


\section{Introduction \label{sect:intro}}

{\bf THERE IS ONLY ONE OF THESE YOU PROBABLY WANT AN STS}


\subsection{Objectives \label{sect:objectives}}

The Software Test Plan describes the system being tested, summarising the system context and decomposition. It sets out the test and verification approach for the system and describes constraints and limitations in the testing to be performed. The STP describes the unit and integration tests for the component modules of the system and describes the validation tests to be performed on the fully integrated system. 

\subsection{Scope \label{sect:scope}}

The Software Test Plan is to be executed by the CU prior to delivery to the DPC where the system will be operated. The DPC will execute integration and acceptance test involving this system within the context of the DPC processing systems.
This document will be updated during the different Gaia cycle phases, according to the requirements updates.

\subsection{Assumptions}  
This paragraph is optional. It describes the preliminary assumptions on which the overall testing strategy are based.
 
\subsection{Applicable Documents \label{sect:ad}}
When applicable documents change a change may be required in this document.
\begin{tabbing}
AUTH-NUM\= \kill 
\citell{LL:TL-001}\>	DPAC Product Assurance Plan \\
\citell{LL:AUTH-XXX} \>	Software Development Plan for \CU \\
\citell{LL:AUTH-XXX}\>	Software Requirements Specification for \product,\\
% perhaps \citell{LL:AUTH-code}\>	Software Requirements Specification for \CU,\\
\end{tabbing}

\subsection{Reference Documents}

\renewcommand{\refname}{}
\bibliographystyle{gaia_aa}
\bibliography{gaia_livelink_valid,gaia_drafts,gaia_refs,gaia_books,gaia_refs_ads}

\subsection{Definitions, acronyms, and abbreviations \label{sect:acronyms}} 
% include acronyms.tex generated by the acronyms.csh (GaiaTools)
\input{acronyms}

\section{Test Items}

The test items covered in this test plan are \product \ and its consituent components:

\begin{itemize_single}
\item All the produsct - from KT diagrams

\item Interfaces  
\item Procedures like Data release 
\end{itemize_single}


\section{Roles and Reporting}

Tester report issues through Mantis, but what other mechanisms will be used?

\subsection{Pass/Fail Criteria}

The Software Review Board will meet once a full run of all Test Cases has been performed, and subsequently after a complete run of all outstanding Test Cases.

A Test Case will be considered ``Passed'' when:
\begin{itemize_single}
\item All of the test steps of the Test Case are completed and
\item All open SPRs from this Test Case agreed in Software Review Board are considered noncritical.
\end{itemize_single}

A Test Case will be considered ``Partially Passed'' when:
\begin{itemize_single}
\item Only a subset of all of the test steps in the Test Case are completed but the overall purpose of the test has been met and
\item Any critical SPRs from this Test Case agreed in Software Review Board are still not closed.
\end{itemize_single}

A Test Case will be considered ``Failed'' when:
\begin{itemize_single}
\item Only a subset of all of the test steps in the Test Case are completed and the overall purpose of the test has not been met and
\item Any critical SPRs from this Test Case agreed in Software Review Board are still not closed.
\end{itemize_single}

\section{Constraints and Limitations}

Describes the limitations and the constraints which apply to CU level tests of the system. lack of computing resources may mean that datasets are smaller or that full accuracy cannot be achieved. Explain what must be validated in the DPC tests

\section{Master Schedule}

The schedule for testing the system until launch. If some modules are scheduled for development after other, explain dependencies and impact on integration and validation tests.

\section{Validation Tools}
\subsection{Introduction}

To evaluate the correctness of the generated data and the systems performances a set of tools may be developed or used. These
tools will provide the means to facilitate the validation tasks. 
Following subsections describe the various tools that can used in the \product validation (e.g. data comparison tools, analysis tools, etc).

\subsection{Data Comparison Tools}
This type of test tools are used to manage products in terms of:
\begin{itemize_single}
\item Comparison of a product generated during a test execution w.r.t. the relevant reference product
\item Non regression verification comparing output products generated by different versions of the same system
\item Measurement of quality degradation due to perturbed inputs
\end{itemize_single}
It allows:
\begin{itemize_single}
\item Product analysis
\item Decoding of generated product allowing to read the most significant data of the product itself
\item Visualisation of the values of a single selected field
\item Apply an accuracy to the comparison
\item Comparing specific parts of the products
\item Filtering using flags values
\end{itemize_single}

\subsection{Data Transformation Tools}
These tools allow the data to be transformed to other formatted data.

\subsection{Analysis Tools}
Descriptions of the performance monitoring tools, profilers, test coverage programs... used in the Performance evaluation tests.\\
...

\section{Unit and Integration Tests}

\subsection{Approach}

Unit and Integration Tests will be automatically executed through the JUnit test framework. The descriptions of the test below are extracted from the test cases code and documentation.The results of Unit and Integration Test to be included in the Sofwtare Test Report will be generated automatically from the output of the execution of the tests by JUnit. A script will be provided to perform thes processing steps.

Module identification? (module tag in class header? mapping file?)

\subsection{Test Coverage}

Test coverage goal for unit and integration testing. Each class and public method shall have a JUnit test harness that may be labelled according to their purpose (e.g. I/O, individual class/method tests, software integration, data model integration etc.). Nominal and contingency
tests should be clearly identified.

Interface coverage...

The tool [insert name of unit test coverage tool here] will be used to provide metrics on the code coverage by Unit Tests for \product \ and this metric will be provide in the Test Report.

\subsection{Unit and Integration Test Specification}

This is a example test plan record; this should be generated automatically.

\begin{longtable} {|p{0.2\textwidth}|p{0.2\textwidth}|p{0.6\textwidth}|}\hline
{\bf Class} & {\bf Unit Test Name} & {\bf Purpose}  \\\hline
Unit Test Class & 
Unit Test Method & 
Purpose of Unit Test from method header \\\hline
\end{longtable}

\section{Validation Tests}

\subsection{General strategy}

Description of the general verification and validation strategy, decomposition into verification testing categories (e.g. science tests, SP external interface tests, algorithms interrelation and sequence). Assessed validation tests results shall be available over the software development duration: they are stored into SVN repository along with related input data, property-file, etc.

A subset of tests are run at DPC during software release qualification process, the results of DPC runs are compared with corresponding test outputs. During DPC integration tests, these assessed outputs will also allow to verify software non-regression.
 
\subsection{Test Designs}

\subsubsection{Test Design \CU-\product-SYS-X}

\paragraph{Objective}

Explain the objective of this test design

\paragraph{Features to be tested}

\begin{itemize_single}
\item Component A
\item Component B
\end{itemize_single}

\paragraph{Features not to be tested}

\begin{itemize_single}
\item Component C
\item Component D
\end{itemize_single}

\paragraph{Approach}

Description of the approach to writing this test design

\paragraph{Test Cases} 

List of test cases to be specified

\begin{longtable} {|p{0.4\textwidth}|p{0.6\textwidth}|}\hline
{\bf Test Case}  & {\bf Description}  \\\hline
\CU-\product-SYS-X-1 & 
Description of Validation Test \\\hline
\end{longtable}

\subsection{Test Case Specification}

\subsubsection{Test Case \CU-\product-SYS-X-1}

\paragraph{Testable Items}

List the components to be tested in this test case




\paragraph{Purpose}

Explain the purpose of this test case

\paragraph{Input Specification}

Describe the inputs to this test (data, written procedures, etc.)

\paragraph{Output Specification}

Describe the outputs of this test

\paragraph{Environment}

Describe the environment (computing resources etc) required for this test.

\paragraph{Inter-case dependencies}

If this test in dependent on another test having been completed successfully (for input data for example), state that here.

\paragraph{Test Procedure}

Describe the procedure to be performed

\paragraph{Test Verification}

Describe how to verify if the test has been successful.

\subsection{Traceability to Requirements}

The traceability between the Requirements describing this system (at SRS or higher level) and the Validation Test Cases should be given here. A script will be provided to create this.

