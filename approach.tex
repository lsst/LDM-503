\section{Verification Tests \label{sect:approach}}

We regard the system as being successfully completed when all of the high level requirements placed upon it, as defined in \citeds{LSE-61} --- the \emph{Data Management System Requirements} --- have been verified.
The approach which will be taken to verifying each requirement is described in \citeds{LDM-639}, the \emph{DM Acceptance Test Specification}.
This test specification covers all aspects of the tests, as described in \secref{sect:tsform}.
Any given requirement may have multiple test cases associated with it in the specification, and these tests will be phased to account for incremental delivery depending on the need for certain functionality at a specific time.
We anticipate that this phasing will align with the priorities assigned to requirements in \citeds{LSE-61} --- that is, that high priority requirements will be verified first --- but this is not, in itself, required, nor will it always be possible.

In addition to the high level requirements on the overall \product{} system, lower-level requirements documents describe requirements placed upon specific parts of the system (for example, \citeds{LDM-554} provides requirements on the LSST Science Platform, and \citeds{LDM-555} on the DM database system).
In general, these low-level requirements are not accompanied by test specification documents; rather, we regard all low-level requirements as being satisfied when the corresponding high-level requirements have been successfully verified according to the \citeds{LDM-639} test specification.
An important exception to this is the Science Platform, for which a detailed test specification --- \citeds{LDM-540} --- is available.
This is appropriate given the wide scope of, and detailed requirements on, this particular component of the DM system.
Given this, the high-level (\citeds{LDM-639}) test activities for the Science Platform may be just inspections of the lower-level test results.

Although individual test cases may be executed at any time, it is anticipated that major testing campaigns will be undertaken to demonstrate the successful completion of major milestones in the \product{} construction effort.
The schedule for these milestones is shown in \secref{sect:schedule}, while \secref{sect:milestones} provides further details as to the contents of each one.
In addition, each low level component owner can define specific test campaigns to verify compliance with the relevant requirements.
For example, such a test campaign may be associated with a software release made for a specific purpose.

\subsection{Managing and Reporting Test Execution}
\label{sect:reports}

As described above, requirements and test specifications are provided in baselined documents.
These documents provide curated views on the Jira \emph{LSST Verification and Validation} project which underlies the LSST-wide test effort.
The Jira system provides ``test scripts'' that testers will follow when carrying out tests, and tracks information about test execution and results.
This information enables us to generate reports on the execution of each test, and ultimately to build a Verification Control Document (VCD; see \figref{fig:doctree} \citeds{LDM-692}).
The VCD will provide the verification status of each DM requirement (in terms of the fraction of test cases pertaining to that requirement which have been successfully executed).

\begin{figure}
\begin{center}
 \includegraphics[angle=-90,width=0.65\textwidth]{images/DocTree}

 \caption{The \product{} document tree.}
 \label{fig:doctree}

\end{center}
\end{figure}

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=0.9\textwidth]{images/DMSDeployment}
		\caption{DM components as deployed during Operations. For details, refer to \citeds{LDM-148}.
		\label{fig:dmsdeploy}}
	\end{center}
\end{figure}

\subsection{Components Under Test}
\label{sect:components}

The components of the DM system are outlined in \citeds{LDM-294} and described in detail in \citeds{LDM-148}; a summary is shown in \figref{fig:dmsdeploy}.

In addition, the test plan presented in this document covers:

\begin{itemize}

\item{The external interfaces between \product{} and other LSST systems.
These are described in \href{https://ls.st/Collection-5201}{DocuShare collection 5201}.
These verification activities will contribute to the global coverage of LSST requirements, but will not be included in the generation of the DM VCD.}

\item{Operational procedures, such as the Data Release Production process.
These are addressed by means of \textit{operations rehearsals} which constitute some of the level two milestones.}

\end{itemize}

The tests associated with each milestone may encompass more than one component of the DM system.
The relevant components must therefore be included in the description of the milestone in \secref{sect:milestones}.

\subsection{Test Approach Overview}\label{sect:tsform}

This section gives an overview of the approach, facilities and documents involved in the verification process.

\subsubsection{Tools}

Properly understanding the test management system requires some familiary with the tooling involved.

\begin{description}

\item[MagicDraw] \hfill \\
MagicDraw\footnote{\url{https://www.nomagic.com/products/magicdraw}} is the standard requirements modeling tool in use by LSST; it is where all requirements are ultimately defined.
The LSST Systems Engineering team use MagicDraw to track verification of the entire LSST system; it is therefore imperative that all results generated by DM are collected here.

\item[Jira and Adaptavist Test Management] \hfill \\
Jira\footnote{\url{https://jira.lsstcorp.org/}} is the issue tracking and management system in use across LSST.
Adaptavist Test Management\footnote{\url{https://www.adaptavist.com/atlassian-apps/test-management-for-jira/}} augments Jira with the capability to manage verification activities.

\item[Syndeia] \hfill \\
Syndeia\footnote{\url{http://intercax.com/products/syndeia/}} is used to synchronize the MagicDraw and Jira systems.

\item[Extraction Scripts] \hfill \\
The LSST team has written a set of scripts to extract information from Jira and format it as test specifications, test plans and reports, and the verification control document.

\end{description}

In general, those who wish to understand the current status of DM should need only to interact with baselined test specifications, test reports and the verification control document: use of the tools described above should not be necessary.

Product owners and individuals carrying out tests will interact with Jira.

Only members of the DM Systems Engineering Team (\citeds{LDM-294}) will interface directly with MagicDraw.

\subsubsection{Requirements and Test Objects}
\label{sect:testobjects}

This section provides an overview of the key concepts and vocabulary used in the test system.

\begin{description}

\item[Requrement] \hfill \\
Requirements are defined in MagicDraw and then synchronized with Jira.
For change control and for distribution to the wider project, they are extracted from MagicDraw to baselined documents.
The level of change control applied to each requirement depends on which document it appears in: high level requirements appear in \citeds{LSE-61} and are subject to project-level change control, while lower level requirements appear in LDM-series documents and are managed by the DM-CCB.

\item[Verification Element] \hfill \\
Each requirement is decomposed into one or more verification elements.
A verification element is an aspect of the requirement which can be independently tested.
Verification elements are created and updated in MagicDraw and synchronized with Jira; in Jira, it appears as a normal issue with a specific type.

\item[Test Case] \hfill \\
A test case is the definition of a procedure to be executed to test the related verification elements.
A single test case may test many verification elements.
Test cases are represented as special objects in Jira provided by the ATM system.

\item[Test Cycle] \hfill \\
A test cycle is list of test cases to be carried out in a particular order under a specified environtment to achieve some specific goal.
Each test cycle may contain only one instance of a particular test case: re-executing a test case with (e.g.) a different configuration must be done in a separate cycle.
Test cycles are represented as special objects in Jira provided by the ATM system.

\item[Test Plan] \hfill \\
A test plan defines the overall plan for achieving some particular goal, such as verifying a software release or completing a milestone (\secref{sect:milestones}).
Each test plan may include several test cycles.
Test plans are represented as special objects in Jira provided by the ATM system.

\item[Test Player] \hfill\\
The Test Player is an interactive Jira tool which provides the tester (\secref{sect:roles}) with instructions and collects responses while a test case is being executed.

\item[Software Problem Report] \hfill \\
A Software Problem Report (SPR) describes a software failure or bug encountered when executing a test case.
SPRs are represetned by Jira tickets in the ``Data Management'' Jira project.

\item[Deviation] \hfill \\
When testing establishes that it is impossible to meet a particular requirement, a Jira issue of type ``Deviation'' is filed.
This ultimately represents a request to change the baseline to remove or relax the requirement in question.
Such a change can only be made with the approval of the relevant (Project or DM) Change Control Board.

\item[Test Campaign] \hfill \\
A test campaign is the sum of all activities needed to plan, execute and report the testing carried out with a specific goal in mind (e.g.\ addressing a particular milestone).
All information relevant for a test campaign is collected in the test plan and related test cycle(s).
Each test plan and report refers to a specific test campaign.

\end{description}

More details on the various Jira objects and detailed instructions on their use are available on Confluence\footnote{https://confluence.lsstcorp.org/display/DM/DM+Test+Approach}.

Workflows for the different types of objects are described in project-level Systems Engineering documentation\footnote{\url{https://confluence.lsstcorp.org/display/SYSENG/LSST+Verification+Architecture}}.

\subsubsection{Test Documents}
\label{sect:testdocs}

Though all test information is contained in MagicDraw and Jira, it is important to have baselined test documentation in Docushare.

The \textbf{Test Specification} for a component collects all of the test cases that cover that particular component.
Test specifications are subject to approval by the responsible change control board, and therefore from part of the project baseline.
However, they are also living documents: as test cases are added or updated in Jira, new editions of the document will be produced, basedlined and provided through Docushare.

Test Specifications consist of a mixture of material which are directly written and curated in GitHub by the corresponding component owner, and sections which are automatically generated from the contents of Jira.

The \textbf{Test Plan and Report}\footnote{For historical reasons, test plan and report documents use the handle ``DMTR''.}  describes all the information related to a particular test cycle --- that is, they describe the contents of the test cycle and the results of executing it.
Since test plans and reports describe the results of a particular campaign, updates are limited to minor corrections (for example, to spelling).
A new test campaign, even one which repeated the same test plan, would result in the creation of a new document to describe the new results.

\subsubsection{Approval Procedure}

Test specifications are part of the project baseline.
As such, they must be approved by the relevant change control board (project-level for LSE-handled documents such as \citeds{LSE-61}; DM-level for LDM-handles).

New test cases, which are not yet part of an accepted specification, or test cases which have been updated since the relevant specification was baselined, will be in the ``draft'' status.
When the relevant specification is accepted, they should be moved to ``approved'' status, at which point the specification document is regenerated and placed in Docushare.

Test cases may also be removed from the baseline in an analagous process.
These test cases should be marked as ``deprecated''.

Test plan and report documents are not change controlled (indeed, per \S\ref{sect:testdocs} they should not be changed).
However, they should be approved by whoever is responsible for requesting that the test be carried out.
For high-level milestones (i.e., those listed in \secref{sect:milestones}), that should be taken to mean the combination of the DM Project Manager and the DM Subsystem Scientist.
Note that approval for these documents must be sought in two stages:

\begin{itemize}

  \item{The test plan should be approved before the test campaign is carried out;}
  \item{The results should be approved after the campaign has been completed.}

\end{itemize}

\subsubsection{The DM VCD}

A global, project-wide verification control document will be derived from MagicDraw by the Systems Engineering team.
However, per \secref{sect:reports}, a Data Management-specific VCD is also be provided in \citeds{LDM-692}.
This will show the test coverage of each requirement its relationship to test cases and their execution.


\subsection{Roles and Personnel}
\label{sect:roles}

Each test case is assigned an \emph{owner}, who is responsible for defining and maintaining it.

Executing the test case is the responsibility of the \emph{tester}, who may be different from the owner.
A given test case may be executed as part of multiple test campaigns; each time, it may be the responsibility of the a different tester.
Test cases are executed following the script provided in the Jira ``LSST Verification and Validation'' project (\secref{sect:tsform}).

Testers have to report the test execution details into the corresponding fields provided in Jira by the ``test player'',  so they can be used to generate test reports.
The information captured in Jira will also be used to populate the Verification Control Document (see \secref{sect:approach}).

\subsection{Success Criteria}

Test cases will sometimes fail.
A test case may be re-run several times until it passes, but the tester must log an explanation than indicates that any failures were understood (e.g.\ they were due to a fault that was fixed) or repeated sufficient times to ensure that passing the test was not a transient success.

Issues which cannot be resolved by the tester in the course of carrying out the test will be reported as ``Software Problem Reports'' (SPRs) through the \product{} ticketing system (the Jira ``Data Management'' project at the time of this document revision).

The SPR describes an issue with the component being tested.
In some cases, that issue may simply be a ``bug'', for which a fix can be implemented as part of regular \product{} development.
The test case can then be re-executed successfully.

In other cases, the SPR may be raised because the component under test is simply incapable of hitting the requirements placed upon it: it is not sufficiently fast, or accurate, or is in some other way deficient.
Ultimately, it may be impossible (either due to resource constraints, or simply because the requirement is unrealistic) to resolve the SPR in such a way that the requirement can be met.
In this case, an issue of type ``Deviation'' may be filed: this represents a request to change or relax the requirements.
Product owners are responsible for reviewing SPRs relating to components which they are responsible for and filing Deviations when appropriate.

A test case cannot be regarded as passing while there are open SPRs preventing its execution.
If the SPRs cannot be resolved in a timely fashion, the test case should be recorded as a failure.
It may be re-executed as part of a fresh test campaign when the SPRs have been resolved.

\subsection{Constraints and Limitations}

\subsubsection{Procedural and Technical Limitations}

\begin{itemize}


  \item{The \product{} system must be verified before the complete LSST system can be completed. Verification is therefore carried out using precursor datasets~\footnote{e.g. from Hyper Suprime-Cam; \secref{LDM-503-02}}, simulated data, and --- where available --- with engineering and pre-release data from the as yet incomplete LSST system~\footnote{e.g. from AuxTel, ComCam and LSSTCam commissioning activities;  \secref{LDM-503-08}, \secref{LDM-503-12a}}.
  Full verification of the \product{} system will depend critically on the availability of this data; if appropriate data is not available, it may not be possible to successfully complete verification of all the high-level requirements defined in \citeds{LSE-61} according to the specified prioritization, or at all before completion of the DM Construction Project.
  In such cases, requirements will be verified as best they can with the available data.
  The Test Plan and Report documents (\secref{sect:testdocs}) will make clear all deviations from the requirements due to the available data as well as any exercises that should be carried out once the necessary data becomes available to fully verify the requirement.
  }

  \item{Metric measurements and operational rehearsals during construction may not involve critical operational systems that are still in development. For example, while computational performance is being measured, computationally dominant algorithmic steps such as deblending and multi-epoch fitting may only be modeled, since they have not yet been implemented; operational rehearsals are done without the factory LSST workflow system; etc.}

\end{itemize}

\subsubsection{Requirements Traceability Constraints}

The \product{} verification plan is based entirely on requirements captured in the DM System Requirements (\citeds{LSE-61}).
It does not refer to higher level requirements documentation, such as the LSST System Requirements (\citeds{LSE-29}) or the Observatory System Specifications (\citeds{LSE-30}); rather, we assume that all higher level requirements have been correctly flowed down to DM.
In practice, the Systems Engineering team continues to refine the flow-down of higher level requirements and issue updates to \citeds{LSE-61}; this test plan must both anticipate and be responsive to those updates.
