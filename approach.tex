\section{Verification Tests \label{sect:approach}}

Our approach towards verifying the successful delivery of the \product{} System follows standard engineering practice.

We regard the system as being successfully completed when all of the high level requirements placed upon it, as defined in \citeds{LSE-61} --- the \emph{Data Management System Requirements} --- have been verified.
The approach which will be taken to verifying each requirement is described in \citeds{LDM-639}, the \emph{DM Acceptance Test Specification}.
This test specification covers all aspects of the tests, as described in \secref{sect:tsform}.
Any given requirement may have multiple test cases associated with it in the specification, and these tests will be phased to account for incremental delivery depending on the need for certain functionality at a specific time.
We anticipate that this phasing will align with the priorities assigned to requirements in \citeds{LSE-61} --- that is, that high priority requirements will be verified first --- but this is not, in itself, required.

In addition to the high level requirements on the overall \product{} system, lower-level requirements documents describe requirements placed upon specific parts of the system (for example, \citeds{LDM-554} provides requirements on the LSST Science Platform, and \citeds{LDM-555} on the DM database system).
Each of these requirements documents is accompanied by a test specification (\citeds{LDM-540} and \citeds{LDM-552} in the case of the previous examples; see also Table \ref{tab:testspecs}), with the relationship between them being the same as for the high-level requirements.
These lower-level requirements documents and test specifications are defined on an as-needed basis: some \product{} components are adequately specified and tested by the high-level documentation.

In general, in order to avoid duplication of test definition and execution, when a requirement is flowed down to lower-level requirements, and therefore tested at lower level, the high level test activity may be just an inspection.
Detailed high level test cases are required if an LSE-61 requirement is not decomposed in lower-level component requirements.

Although individual test cases may be executed at any time, it is anticipated that major testing campaigns will be undertaken to demonstrate the successful completion of major milestones in the \product{} construction effort.
The schedule for these milestones is shown in \secref{sect:schedule}, while \secref{sect:milestones} provides further details as to the contents of each one.
In addition, each low level component owner can define specific test campaigns to verify compliance with the relevant requirements.
For example, such a test campaign may be associated with a software release made for a specific purpose.

\subsection{Managing and Reporting Test Execution}
\label{sect:reports}

As described above, requirements and test specifications are provided in baselined documents.
These documents provide curated views on the Jira \emph{LSST Verification and Validation} project which underlies the LSST-wide test effort.
The Jira system provides ``test scripts'' that testers will follow when carrying out tests, and tracks information about test execution and results.
This information enables us to generate reports on the execution of each test, and ultimately to build a Verification Control Document (VCD; see \figref{fig:doctree}).
The VCD will provide the verification status of each DM requirement (in terms of the fraction of test cases pertaining to that requirement which have been successfully executed).

\begin{figure}
\begin{center}
 \includegraphics[angle=-90,width=0.65\textwidth]{images/DocTree}

 \caption{The \product{} document tree.}
 \label{fig:doctree}

\end{center}
\end{figure}

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=0.9\textwidth, trim={0cm 15cm 0cm 0cm}]{images/DMSDeployment}
		\caption{DM components as deployed during Operations. For details, refer to \citeds{LDM-148}.
		\label{fig:dmsdeploy}}
	\end{center}
\end{figure}

\subsection{Components Under Test}
\label{sect:components}

The components of the DM system are outlined in \citeds{LDM-294} and described in detail in \citeds{LDM-148}; a summary is shown in \figref{fig:dmsdeploy}.
The test specifications covering these components are shown in \tabref{tab:testspecs}, but note that, at time of writing, the document tree is being refactored and document numbers are not currently available for all components.

In addition, the test plan presented in this document covers:

\begin{itemize}

\item{The external interfaces between \product{} and other LSST systems.
These are described in \href{https://ls.st/Collection-5201}{DocuShare collection 5201}.
These verification activities will contribute to the global coverage of LSST requirements, but will not be included in the generation of the DM VCD.}

\item{Operational procedures, such as the Data Release Production process.
These are addressed by means of \textit{operations rehearsals} which constitute some of the level two milestones.}

\end{itemize}

\input{TopLevelTestSpecs}

The tests associated with each milestone may encompass more than one component of the DM system.
The relevant components must therefore be included in the description of the milestone in \secref{sect:milestones}.

\subsection{Test Approach Overview}\label{sect:tsform}

This section gives an overview of the approach, facilities and documents involved in the verification process.

\subsubsection{Tools}

Properly understanding the test management system requires some familiary with the tooling involved.

\begin{description}

\item[MagicDraw] \hfill \\
MagicDraw\footnote{\url{https://www.nomagic.com/products/magicdraw}} is the standard requirements modeling tool in use by LSST; it is where all requirements are ultimately defined.
The LSST Systems Engineering team use MagicDraw to track verification of the entire LSST system; it is therefore imperative that all results generated by DM are collected here.

\item[Jira and Adaptavist Test Management] \hfill \\
Jira\footnote{\url{https://jira.lsstcorp.org/}} is the issue tracking and management system in use across LSST.
Adaptavist Test Management\footnote{\url{https://www.adaptavist.com/atlassian-apps/test-management-for-jira/}} augments Jira with the capability to manage verification activities.

\item[Syndeia] \hfill \\
Syndeia\footnote{\url{http://intercax.com/products/syndeia/}} is used to synchronize the MagicDraw and Jira systems.

\item[Extraction Scripts] \hfill \\
The LSST team has written a set of scripts to extract information from Jira and format it as test specifications, test plans and reports, and the verification control document.

\end{description}

In general, those who wish to understand the current status of DM should need only to interact with baselined test specifications, test reports and the verification control document: use of the tools described above should not be necessary.

Product owners and individuals carrying out tests will interact with Jira.

Only members of the DM Systems Engineering Team (\citeds{LDM-294}) will interface directly with MagicDraw.

\subsubsection{Requirements and Test Objects}
\label{sect:testobjects}

This section provides an overview of the key concepts and vocabulary used in the test system.

\begin{description}

\item[Requrement] \hfill \\
Requirements are defined in MagicDraw and then synchronized with Jira.
For change control and for distribution to the wider project, they are extracted from MagicDraw to baselined documents.

\item[Verification Element] \hfill \\
Each requirement is decomposed into one or more verification elements.
A verification element is an aspect of the reuqirement which can be independently tested.
Verification elements are created and updated in MagicDraw and synchronized with Jira; in Jira, it appears as a normal issue with a specific type.

\item[Test Case] \hfill \\
A test case is the definition of a procedure to be executed to test the related verification elements.
A single test case may test many verification elements.
Test cases are represented as special objects in Jira provided by the ATM system.

\item[Test Cycle] \hfill \\
A test cycle is list of test cases to be carried out in a particular order under a specified environtment to achieve some specific goal.
Each test cycle may contain only one instance of a particular test case: re-executing a test case with (e.g.) a different configuration must be done in a separate cycle.
Test cycles are represented as special objects in Jira provided by the ATM system.

\item[Test Plan] \hfill \\
A test plan defines the overall plan for achieving some particular goal, such as verifying a software release or completing a milestone (\secref{sect:milestones}).
Each test plan may include several test cycles.
Test plans are represented as special objects in Jira provided by the ATM system.

\item[Deviation] \hfill \\
When a test failure renders it impossible to successfully verify a verification element, a Jira issue of type ``deviation'' should be opened to capture this outcome.

\end{description}

More details on the various Jira objects and detailed instructions on their use are available on Confluence\footnote{https://confluence.lsstcorp.org/display/DM/DM+Test+Approach}.

Workflows for the different types of objects are described in project-level Systems Engineering documentation (\url{https://confluence.lsstcorp.org/display/SYSENG/LSST+Verification+Architecture}).

\subsubsection{Test Documents}
\label{sect:testdocs}

Though all test information is contained in MagicDraw and Jira, it is important to have baselined test documentation in Docushare.

The \textbf{Test Specification} for a component collects all of the test cases that cover that particular component.
Test specifications are subject to approval by the responsible change control board, and therefore from part of the project baseline.
However, they are also living documents: as test cases are added or updated in Jira, new editions of the document will be produced, basedlined and provided through Docushare.

Test Specifications consist of a mixture of material which are directly written and curated in GitHub by the corresponding component owner, and sections which are automatically generated from the contents of Jira.

The \textbf{Test Plan and Report}\footnote{For historical reasons, test plan and report documents use the handle ``DMTR''.}  describes all the information related to a particular test ``campaign'' --- that is, they describe the contents a test plan and the results of executing it.
Since test plans and reports describe the results of a particular campaign, updates are limited to minor corrections (for example, to spelling).
A new test campaign, even one which repeated the same test plan, would result in the creation of a new document to describe the new results.

\subsubsection{Approval Procedure}

Test specifications are part of the project baseline.
As such, they must be approved by the relevant change control board (project-level for LSE-handled documents such as \citeds{LSE-61}; DM-level for LDM-handles).

New test cases, which are not yet part of an accepted specification, or test cases which have been updated since the relevant specification was baselined, will be in the ``draft'' status.
When the relevant specification is accepted, they should be moved to ``approved'' status, at which point the specification document is regenerated and placed in Docushare.

Test cases may also be removed from the baseline in an analagous process.
These test cases should be marked as ``deprecated''.

Test plan and report documents are not change controlled (indeed, per \S\ref{sect:testdocs} they should not be changed).
However, they should be approved by whoever is responsible for requesting that the test be carried out.
For high-level milestones (i.e., those listed in \secref{sect:milestones}), that should be taken to mean the combination of the DM Project Manager and the DM Subsystem Scientist.
Note that approval for these documents must be sought in two stages:

\begin{itemize}

  \item{The test plan should be approved before the test campaign is carried out;}
  \item{The results should be approved after the campaign has been completed.}

\end{itemize}

\subsubsection{The DM VCD}

A global, project-wide verification control document will be derived from MagicDraw by the Systems Engineering team.
However, per \secref{sect:reports}, a Data Management-specific VCD will also be provided.
This will show the test coverage of each requirement on the subsystem and its relationship to test cases and test campaigns.
A conceptual example of the structure of the VCD is given in \tabref{tab:dmvcd}.

\input{DMVcdExample}

The table is organized as follows:

\begin{itemize}

  \item{The font size is made in order to put in evidence a specific Requirement, the corresponding verification element, the test case and the last execution information;}
  \item{Parent and child reuirement are represented using smaller font size, and now additional information is provided for them}

\end{itemize}

What is provided here is just an example. When the DM VCD will be produced, the layout, including spacing and font size, can be different.

\subsection{Roles and Personnel}
\label{sect:roles}


For each test case is required to specify the ``owner'', that is in charge to write and maintain the test case.

The \emph{tester} in charge of executing the test case, will be specified when a test campaign is instantiated and can be different from the ``owner''.

The tester will execute the test cases following the script provided in the Jira ``LSST Verification and Validation'' project (\secref{sect:tsform}).

Testers have to report the test execution details into the corresponding fields provided in Jira by the ``test player'',  so they can be used to generate test reports.
The information captured in Jira will also be used to populate the Verification Control Document (see \secref{sect:approach}).

Tests and procedures will sometimes fail: a test case may be re-run several times until it passes, but testers will log an explanation than indicates that any failures were understood (e.g.\ they were due to a fault that was fixed) or repeated sufficient times to ensure that passing the test was not transient success.
Issues which cannot be resolved by the tester in the course of carrying out the test will be reported as ``Software Problem Reports'' (SPRs) through the \product{} ticketing system (the Jira ``Data Management'' project at the time of this document revision). 
These SPRs are issues related to the software product under test.
The DM-CCB, or an individual designated by it, will be tasked with assessing the SPRs determining the timescale for re-executing the test procedure.
If a new version of the software is provided, a new test cycle can be create in order to document the new run in a separate Jira element, and keep track of the history.

Other parties that have a relevant role in \product\ verification are identified in the appropriate sections of the document; these are involved in their primary capacity (e.g.\ the DM Systems Engineer) and so are not individually listed in this section.

\subsection{Pass/Fail Criteria}

A test case will be considered ``Passed'' when:

\begin{itemize_single}
\item{All of the test steps of the Test Case are completed; and}
\item{All open SPRs from this Test Case are considered noncritical by DMCCB.}
\end{itemize_single}

A test case will be considered ``Partially Passed'' when:

\begin{itemize_single}
\item{Only a subset of all of the test steps in the Test Case are completed and/or there remain open SPRs which are regarded as critical by the DMCCB; but}
\item{The DMCCB regards overall purpose of the test as having been met.}
\end{itemize_single}

A test case will be considered ``Failed'' when:

\begin{itemize_single}
\item{Only a subset of all of the test steps in the Test Case are completed and/or there remain open SPRs which are regarded as critical by the DMCCB; and}
\item{The DMCCB regards overall purpose of the test as not having been met.}
\end{itemize_single}

Note that in \citeds{LPM-17} science requirements are described as having a minimum specification, a design specification and a stretch goal.
While we preserve these distinctions in some DM tooling for internal tracking purposes, for the purposes of these tests, it is the design specification that is verified as having been met for a test to pass without intervention of the DMCCB.
Ultimately, if it proves impossible to satisfy a requirement at design specification, LSST Project level approval is required to accept the minimum specification.
This has to be done using a Deviation issue.

\subsection{Constraints and Limitations}

\subsubsection{Procedural and Technical Limitations}

\begin{itemize}


  \item{The \product{} system must be verified before the complete LSST system can be completed. Verification is therefore carried out using precursor datasets~\footnote{e.g. from Hyper Suprime-Cam; \secref{LDM-503-02}}, simulated data, and --- where available --- with engineering and pre-release data from the as yet incomplete LSST system.}

  \item{Metric measurements and operational rehearsals during construction may not involve critical operational systems that are still in development. For example, while computational performance is being measured, computationally dominant algorithmic steps such as deblending and multi-epoch fitting may only be modeled, since they have not yet been implemented; operational rehearsals are done without the factory LSST workflow system; etc.}

\end{itemize}

\subsubsection{Requirements Traceability Constraints}

The \product{} verification plan is based entirely on requirements captured in the DM System Requirements (\citeds{LSE-61}).
It does not refer to higher level requirements documentation, such as the LSST System Requirements (\citeds{LSE-29}) or the Observatory System Specifications (\citeds{LSE-30}); rather, we assume that all higher level requirements have been correctly flowed down to DM.
In practice, the Systems Engineering team continues to refine the flow-down of higher level requirements and issue updates to \citeds{LSE-61}; this test plan must both anticipate and be responsive to those updates.
