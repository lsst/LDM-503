\section{Verification Tests \label{sect:approach}}

Our approach towards verifying the successful delivery of the \product{} System follows standard engineering practice.

We regard the system as being successfully completed when all of the high level requirements placed upon it, as defined in \citeds{LSE-61} --- the \emph{Data Management System Requirements} --- have been verified.
The approach which will be taken to verifying each requirement is described in \citeds{LDM-639}, the \emph{DM Acceptance Test Specification}.
This test specification covers all aspects of the tests, as described in \secref{sect:tsform}.
Any given requirement may have multiple test cases associated with it in the specification, and these tests will be phased to account for incremental delivery depending on the need for certain functionality at a specific time.
We anticipate that this phasing will align with the priorities assigned to requirements in \citeds{LSE-61} --- that is, that high priority requirements will be verified first --- but this is not, in itself, required.

In addition to the high level requirements on the overall \product{} system, lower-level requirements documents describe requirements placed upon specific parts of the system (for example, \citeds{LDM-554} provides requirements on the LSST Science Platform, and \citeds{LDM-555} on the DM database system).
Each of these requirements documents is accompanied by a test specification (\citeds{LDM-540} and \citeds{LDM-552} in the case of the previous examples; see also Table \ref{tab:testspecs}), with the relationship between them being the same as for the high-level requirements.
These lower-level requirements documents and test specifications are defined on an as-needed basis: some \product{} products are adequately specified and tested by the high-level documentation.

In general, in order to avoid duplication of test definition and execution, when a requirement is flowed down to lower-level requirements, and therefore tested at lower level, the high level test activity may be just an inspection.
Detailed high level test cases are required if an LSE-61 requirement is not decomposed in lower-level product requirements.

Although individual test cases may be executed at any time, it is anticipated that major testing campaigns will be undertaken to demonstrate the successful completion of major milestones in the \product{} construction effort.
The schedule for these milestones is shown in \secref{sect:schedule}, while \secref{sect:milestones} provides further details as to the contents of each one.
In addition, each low level product owner can define specific test campaigns to verify compliance with the relevant requirements.
For example, such a test campaign may be associated with a software release made for a specific purpose.

\subsection{Managing and Reporting Test Execution}
\label{sect:reports}

As described above, requirements and test specifications are provided in baselined documents.
These documents provide curated views on the Jira \emph{LSST Verification and Validation} project which underlies the LSST-wide test effort.
The Jira system provides ``test scripts'' that testers will follow when carrying out tests, and tracks information about test execution and results.
This information enables us to generate reports on the execution of each test\footnote{These test reports may, on occasion, be issued as Word or \LaTeX{} documents, but this is not, in general, required.}, and ultimately to build a Verification Control Document (VCD; see \figref{fig:doctree}).
The VCD will provide the verification status of each DM requirement (in terms of the fraction of test cases pertaining to that requirement which have been successfully executed).

\begin{figure}
\begin{center}
 \includegraphics[angle=-90,width=0.65\textwidth]{images/DocTree}

 \caption{The \product{} document tree.}
 \label{fig:doctree}

\end{center}
\end{figure}

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=0.9\textwidth, trim={0cm 15cm 0cm 0cm}]{images/DMSDeployment}
		\caption{DM components as deployed during Operations. For details, refer to \citeds{LDM-148}.
		\label{fig:dmsdeploy}}
	\end{center}
\end{figure}

\subsection{Components Under Test}
\label{sect:components}

The components of the DM system are outlined in \citeds{LDM-294} and described in detail in \citeds{LDM-148}; a summary is shown in \figref{fig:dmsdeploy}.
The test specifications covering these components are shown in \tabref{tab:testspecs}, but note that, at time of writing, the document tree is being refactored and document numbers are not currently available for all components.

In addition, the test plan presented in this document covers:

\begin{itemize}

\item{The external interfaces between \product{} and other LSST systems.
These are described in \href{https://ls.st/Collection-5201}{DocuShare collection 5201}.
These verification activities will contribute to the global coverage of LSST requirements, but will not be included in the generation of the DM VCD.}

\item{Operational procedures, such as the Data Release Production process.
These are addressed by means of \textit{operations rehearsals} which constitute some of the level two milestones.}

\end{itemize}

\input{TopLevelTestSpecs}

The tests associated with each milestone may encompass more than one component of the DM system.
The relevant components must therefore be included in the description of the milestone in \secref{sect:milestones}.

\subsection{Test Approach Overview}\label{sect:tsform}

This section gives an overview of the approach, facilities and documents involved in the verification process.

\subsubsection{Tools}

Properly understanding the test management system requires some familiary with the tooling involved.

\begin{description}

\item[MagicDraw] \hfill \\
MagicDraw\footnote{\url{https://www.nomagic.com/products/magicdraw}} is the standard requirements modeling tool in use by LSST; it is where all requirements are ultimately defined.
The LSST Systems Engineering team use MagicDraw to track verification of the entire LSST system; it is therefore imperative that all results generated by DM are collected here.

\item[Jira and Adaptavist Test Management] \hfill \\
Jira\footnote{\url{https://jira.lsstcorp.org/}} is the issue tracking and management system in use across LSST.
Adaptavist Test Management\footnote{\url{https://www.adaptavist.com/atlassian-apps/test-management-for-jira/}} augments Jira with the capability to manage verification activities.

\item[Syndeia] \hfill \\
Syndeia\footnote{\url{http://intercax.com/products/syndeia/}} is used to synchronize the MagicDraw and Jira systems.

\item[Extraction Scripts] \hfill \\
The LSST team has written a set of scripts to extract information from Jira and format it as test specifications, test plans and reports, and the verification control document.

\end{description}

In general, those who wish to understand the current status of DM should need only to interact with baselined test specifications, test reports and the verification control document: use of the tools described above should not be necessary.

Product owners and individuals carrying out tests will interact with Jira.

Only members of the DM Systems Engineering Team (\citeds{LDM-294}) will interface directly with MagicDraw.

\subsubsection{Requirements and Test Objects}
\label{sect:testobjects}

This section provides an overview of the key concepts and vocabulary used in the test system.

\begin{description}

\item[Requrement] \hfill \\
Requirements are defined in MagicDraw and then synchronized with Jira.
For change control and for distribution to the wider project, they are extracted from MagicDraw to baselined documents.
The level of change control applied to each requirement depends on which document it appears in: high level requirements appear in \citeds{LSE-61} and are subject to project-level change control, while lower level requirements appear in LDM-series documents and are managed by the DM-CCB.

\item[Verification Element] \hfill \\
Each requirement is decomposed into one or more verification elements.
A verification element is an aspect of the reuqirement which can be independently tested.
Verification elements are created and updated in MagicDraw and synchronized with Jira; in Jira, it appears as a normal issue with a specific type.
Updates made in Jira can be propagated back to MagicDraw \textit{with the cooperation of the Systems Engineering Group}\footnote{What's the process for this? Do you file a ticket on SE and ask them to do an update? Or...?}.

\item[Test Case] \hfill \\
A test case is the definition of a procedure to be executed to test the related verification elements.
A single test case may test many verification elements.
Test cases are represented as special objects in Jira provided by the ATM system.

\item[Test Cycle] \hfill \\
A test cycle is list of test cases to be carried out in a particular order under a specified environtment to achieve some specific goal.
Each test cycle may contain only one instance of a particular test case: re-executing a test case with (e.g.) a different configuration must be done in a separate cycle.
Test cycles are represented as special objects in Jira provided by the ATM system.

\item[Test Plan] \hfill \\
A test plan defines the overall plan for achieving some particular goal, such as verifying a software release or completing a milestone (\secref{sect:milestones}).
Each test plan may include several test cycles.
Test plans are represented as special objects in Jira provided by the ATM system.

\item[Deviation] \hfill \\
When a test failure renders it impossible to successfully verify a verification element, a Jira issue of type ``deviation'' should be opened to capture this outcome.

\end{description}

More details on the various Jira objects and detailed instructions on their use are available on Confluence\footnote{https://confluence.lsstcorp.org/display/DM/DM+Test+Approach}.

Workflows for the different types of objects are described in project-level Systems Engineering documentation.


\subsubsection{Test Documents}

Though all tests information is contained in MagicDraw and Jira, it is important to have a  baselined test documentation in Docushare. .

The {\bf Test Specification} document consists of a collections of test cases that cover a specific \product{} product, as defined in the document tree and in \secref{sect:components}.
These collections of test cases are available in Jira as views in the ``LSST Verification and Validation'' project and are organized in folders.

The sections in a test specification document are organized as follows:

\begin{itemize}
\item Introduction: includes subsections ``Objective'' and ``Scope''. To edit and maintain manually by the author uploading changes in github.
\item Approach: the relevant subsections are ``Tasks and criteria'', `` Features to be tested'', ``Features not to be tested'', ``Pass/fail criteria'', ``Suspension criteria'', ``Naming convention''. To edit and maintain manually by the author uploading changes in github.
\item Test Cases Summary: this is a summary table extracted directly from Jira that summarizes all test cases included in the test specification. Do not edit it manually, all changes need to be done in Jira.
\item Test Cases: this section includes all test cases details extracted automatically from Jira. Do not edit it manually, al changes need to be done in Jira.
\item Appendix: Requirements traceability.
\end{itemize}

One additional appendix will be added in order to include in the baselined document the corresponding verification elements.

The test specification is a living document: test cases may be added or updated in Jira and new issues of the same document will be generated, approved and uploaded in Docushare.

Note: never update a test specification in section 3 or 4, since this will be overwritten by the extraction scripts. Note also that the old naming convention is not applicable anymore, since the test case identification is assigned automatically by Jira. Existing test cases will preserve the old identification in the subject.

Taking advantage of the continuous integration process executed in Travis, the {\bf extraction scripts} will be integrated in the build of the pdf in order to extract dynamically the section 3 and 4 content from Jira, on a regular basis, to be defined.


The {\bf Test Plan and Report} is the documented that will include all information related to a test campaign.
This includes all information from the test plan and the related test runs.
Do not edit it directly, all changes need to be done in Jira.

The author need to define the structure of the document and upload it in github. The continuous integration in Travis will take care to build the document on a regular bases, to be defined, downloading the content from Jira.

The test plan and report scope is constrained on a single major test activity. It may only be updated, generating a new issues of the document, in case of re-execution of a subset of tests after the test campaign is concluded and the document is already in Docushare.
In general a new test activity, due to a new milestone or release to be tested, imply a creation of a new test plan and report, with a different document title and code.

Note that in the past this document was identified as "Test Report" but is now
the "Test Plan and Report" in order to better identify the two types of information it contains: planning of the test activity and report on the executed test cases. The document type acronym will remain DMTR.

Note also that, the reports considered in this context are only the documents which scope is to provide information on test campaigns documented and executed using the test management infrastructure in Jira. For example, release characterization reports are not relevant here, unless their documentation, planning and execution follow these guidelines and use the same tooling.


\subsubsection{Approval Procedure}

The approval of the test specification is already addressed via RFC as done for all the DM-CCB controlled documents.

New or updated test cases shall be in status ``draft''. After the test specification has been approved via RFC, their status need to be set to ``approved''.
Only then the document can be regenerated and uploaded in Docushare.

The approval of the test plan and report is a bit different since it is completed in 2 steps:

\begin{itemize}
\item before the test execution: the test plan and the test cycle(s) should be created in Jira, and information should be provided in order to demonstrate that the test is ready to start.
\item after the test execution: the document is finalized including the reports and comments on the execution of the test cases, issues raised during the test campaign and the final statement summarizing the global test campaign result (overall assessment in the test plan in Jira).
\end{itemize}

Only the final document is relevant for deriving the global coverage of the DM requirements, but it is important that both steps are approved separately. In this way we ensure all conditions are met to  execute the tests.

The approval of the first step comes after verification,  by the product owner, that all needed information is provided in Jira and therefore in the test plan and report document.
The formality of  issuing  the document in Docushare can be skipped, but a coherent draft has to be available in DMTR-XXX.lsst.io and the status of the corresponding test plan jira object has to be set to "approved".
Stakeholders that may have some expectation on the activity results, like scientists, T/CAMs or others, should  be involved before the approval.

The final approval of the test plan and report document, after the test activity is concluded and all information is collected in Jira, should be explicitly given by whoever requested the test activity to be carried out.

\subsubsection{The DM VCD}

The global project wide verification control document will be obtained from MagicDraw by the System Engineering team.

However, Data Management will be able to provide a DM specific verification control document, that will show the coverage of each system specific requirement, and their relation with test cases and test executions.
This information will be derived only from Jira using the ``extraction scripts'' already used for the generation of the test specification and test plan and report.

An example, at least in structure, of the VCD is given in \tabref{tab:dmvcd}.

\input{DMVcdExample}

\subsection{Roles and Personnel}
\label{sect:roles}


For each test case is required to specify the ``owner''.
This in general is the \emph{tester} that will be in charge of executing the test case.
However, when assigning a test case to a specific test cycle, it is possible to specify a different name.

The tester is responsible for executing the test cases following the script provided in the Jira ``LSST Verification and Validation'' project (\secref{sect:tsform}).

Testers have to report the test execution details into the corresponding fields provided in Jira by the ``test player'',  so they can be used to generate test reports.
The information captured in Jira will also be used to populate the Verification Control Document (see \secref{sect:approach}).

Tests and procedures will sometimes fail: a test case may be re-run several times until it passes, but testers will log an explanation than indicates that any failures were understood (e.g.\ they were due to a fault that was fixed) or repeated sufficient times to ensure that passing the test was not transient success.
Issues which cannot be resolved by the tester in the course of carrying out the test will be reported as ``Software Problem Reports'' (SPRs) through the \product{} ticketing system (the Jira ``Data Management'' project at the time of this document revision).
The DM-CCB, or an individual designated by it, will be tasked with assessing the SPRs determining the timescale for re-executing the test procedure.
If a new version of the software is provided, a new test cycle can be create in order to document the new run in a separate Jira element, and keep track of the history.

Other parties that have a relevant role in \product\ verification are identified in the appropriate sections of the document; these are involved in their primary capacity (e.g.\ the DM Systems Engineer) and so are not individually listed in this section.

\subsection{Pass/Fail Criteria}

A test case will be considered ``Passed'' when:

\begin{itemize_single}
\item{All of the test steps of the Test Case are completed; and}
\item{All open SPRs from this Test Case are considered noncritical by DMCCB.}
\end{itemize_single}

A test case will be considered ``Partially Passed'' when:

\begin{itemize_single}
\item{Only a subset of all of the test steps in the Test Case are completed and/or there remain open SPRs which are regarded as critical by the DMCCB; but}
\item{The DMCCB regards overall purpose of the test as having been met.}
\end{itemize_single}

A test case will be considered ``Failed'' when:

\begin{itemize_single}
\item{Only a subset of all of the test steps in the Test Case are completed and/or there remain open SPRs which are regarded as critical by the DMCCB; and}
\item{The DMCCB regards overall purpose of the test as not having been met.}
\end{itemize_single}

Note that in \citeds{LPM-17} science requirements are described as having a minimum specification, a design specification and a stretch goal.
While we preserve these distinctions in some DM tooling for internal tracking purposes, for the purposes of these tests, it is the design specification that is verified as having been met for a test to pass without intervention of the DMCCB.
Ultimately, if it proves impossible to satisfy a requirement at design specification, LSST Project level approval is required to accept the minimum specification.

\subsection{Constraints and Limitations}

\subsubsection{Procedural and Technical Limitations}

\begin{itemize}


  \item{The \product{} system must be verified before the complete LSST system can be completed. Verification is therefore carried out using precursor datasets~\footnote{e.g. from Hyper Suprime-Cam; \secref{LDM-503-02}}, simulated data, and --- where available --- with engineering and pre-release data from the as yet incomplete LSST system.}

  \item{Metric measurements and operational rehearsals during construction may not involve critical operational systems that are still in development. For example, while computational performance is being measured, computationally dominant algorithmic steps such as deblending and multi-epoch fitting may only be modeled, since they have not yet been implemented; operational rehearsals are done without the factory LSST workflow system; etc.}

\end{itemize}

\subsubsection{Requirements Traceability Constraints}

The \product{} verification plan is based entirely on requirements captured in the DM System Requirements (\citeds{LSE-61}).
It does not refer to higher level requirements documentation, such as the LSST System Requirements (\citeds{LSE-29}) or the Observatory System Specifications (\citeds{LSE-30}); rather, we assume that all higher level requirements have been correctly flowed down to DM.
In practice, the Systems Engineering team continues to refine the flow-down of higher level requirements and issue updates to \citeds{LSE-61}; this test plan must both anticipate and be responsive to those updates.
